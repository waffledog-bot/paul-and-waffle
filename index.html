<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Paul & Waffle ‚Äî Building with AI</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1>Paul & Waffle üßá</h1>
      <p class="subtitle">A human and an AI building stuff together over WhatsApp.</p>
      <p class="intro">This is a living build log. Every card below is something we actually built, broke, fixed, or figured out ‚Äî starting from a blank MacBook Air and an OpenClaw install. If you're setting up your own AI assistant, steal whatever's useful.</p>
      <p class="intro-links">
        <a href="https://github.com/waffledog-bot/paul-and-waffle" class="header-link">üìÇ GitHub Repo</a>
        <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills" class="header-link">üì¶ All Skills</a>
        <a href="https://openclaw.ai" class="header-link">üêæ OpenClaw</a>
      </p>
    </div>
  </header>

  <main class="container">

    <section class="task" id="the-setup">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">01</span>
          <h2>The Setup</h2>
        </div>
        <p class="task-summary">OpenClaw on a MacBook Air, talking through WhatsApp selfChatMode.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>The whole thing runs on a MacBook Air (M-series, macOS Tahoe 26.2). OpenClaw is the agent framework ‚Äî it gives Claude tools, memory, and the ability to talk to you through messaging apps.</p>
        <h3>Key decisions</h3>
        <ul>
          <li><strong>WhatsApp over Signal:</strong> We went with WhatsApp in <code>selfChatMode</code> ‚Äî the bot uses your own phone number, so you're literally texting yourself. No second phone number needed.</li>
          <li><strong>Workspace as home base:</strong> Everything lives in <code>~/.openclaw/workspace/</code> ‚Äî identity files, memory, skills, projects. The agent reads these on every boot.</li>
          <li><strong>Identity files:</strong> <code>SOUL.md</code> (personality), <code>USER.md</code> (who you are), <code>IDENTITY.md</code> (who the agent is), <code>MEMORY.md</code> (long-term memory). These make the agent feel like <em>your</em> agent, not a generic chatbot.</li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li>Debug messages and tool errors were leaking into WhatsApp. Fixed with <code>messages.suppressToolErrors: true</code> and <code>tools.exec.notifyOnExit: false</code> in config.</li>
          <li>Auto-transcription of voice memos never fired despite correct config. Manual transcription via the whisper-api skill works fine ‚Äî we moved on rather than burning hours debugging.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="image-generation">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">02</span>
          <h2>Image Generation</h2>
        </div>
        <p class="task-summary">Generate images with Gemini ‚Äî Waffle's first act was creating its own avatar.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>The very first thing Waffle did after being born was generate its own self-portrait. Used the <code>nano-banana-pro</code> skill which wraps Google's Gemini image generation.</p>
        <div class="media-showcase">
          <img src="media/waffle-avatar.png" alt="Waffle's self-portrait ‚Äî a golden waffle character with a lightbulb and gears" class="media-image">
          <p class="media-caption">Waffle's self-portrait: "a golden waffle character with lightbulb and gears, creative and warm"</p>
        </div>
        <h3>How it works</h3>
        <ul>
          <li><code>nano-banana-pro</code> skill sends prompts to Gemini's image generation API</li>
          <li>Returns images that can be sent via WhatsApp or saved to files</li>
          <li>Good for avatars, app icons, concept art, visual brainstorming</li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li>Requires a Gemini API key configured in <code>skills.entries.nano-banana-pro.apiKey</code></li>
          <li>Results vary a lot with prompt wording ‚Äî be specific about style, mood, and composition</li>
        </ul>
      </div>
    </section>

    <section class="task" id="voice-memos">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">03</span>
          <h2>Voice Memos</h2>
        </div>
        <p class="task-summary">Send voice messages from the AI via WhatsApp using ElevenLabs TTS.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/voice-memo">voice-memo skill</a></p>
        <p>We wanted Waffle to be able to reply with voice notes ‚Äî not just text. Built a <code>voice-memo</code> skill that generates audio with ElevenLabs and sends it as a WhatsApp voice note.</p>
        <div class="media-showcase">
          <audio controls src="media/voice-memo-test.mp3"></audio>
          <p class="media-caption">Waffle's first voice memo ‚Äî testing the ElevenLabs pipeline</p>
        </div>
        <h3>How it works</h3>
        <ul>
          <li><code>sag</code> CLI generates the audio (ElevenLabs TTS, v3 model)</li>
          <li>Audio saved to workspace dir (not <code>/tmp/</code>!)</li>
          <li>Sent via OpenClaw's message tool with <code>asVoice: true</code></li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li><strong><code>/tmp/</code> is not in allowed media directories.</strong> Files saved there can't be sent via WhatsApp. Always save to the workspace.</li>
          <li>Default voice is "George" (Warm, Captivating Storyteller) ‚Äî good for summaries and recaps.</li>
          <li>ElevenLabs v3 supports audio tags like <code>[whispers]</code>, <code>[excited]</code>, <code>[laughs]</code> for expression.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="stem-splitting">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">04</span>
          <h2>Stem Splitting</h2>
        </div>
        <p class="task-summary">Find a song on YouTube, download it, and split into vocals, drums, bass, and more.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/stem-split">stem-split skill</a></p>
        <p>The <code>stem-split</code> skill ties together <code>yt-dlp</code> (download) and <code>demucs</code> (Meta's AI source separation) into one pipeline. Ask for "the instrumental of Big Iron" and it just works.</p>
        <div class="media-showcase">
          <p class="media-label">üé§ Vocals only</p>
          <audio controls src="media/rf-vocals.mp3"></audio>
          <p class="media-label">üé∏ Instrumental (no vocals)</p>
          <audio controls src="media/rf-instrumental.mp3"></audio>
          <p class="media-caption">ONLAP ‚Äî Nevermind (copyright free), split into vocals and instrumental via Demucs</p>
        </div>
        <h3>The pipeline</h3>
        <ol>
          <li>Find the song: <code>yt-dlp --print title --print webpage_url "ytsearch1:song name"</code></li>
          <li>Download audio: <code>yt-dlp -x --audio-format wav</code></li>
          <li>Split stems: <code>demucs --two-stems vocals -n htdemucs --mp3</code></li>
          <li>Send results via WhatsApp</li>
        </ol>
        <h3>Modes</h3>
        <ul>
          <li><strong>2stems:</strong> vocals + instrumental (karaoke mode)</li>
          <li><strong>4stems:</strong> vocals, drums, bass, other</li>
          <li><strong>5stems:</strong> vocals, drums, bass, piano, other</li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>Spleeter doesn't work on Apple Silicon.</strong> TensorFlow dependency is broken. Use <code>demucs</code> instead ‚Äî it's PyTorch-based and produces better results anyway.</li>
          <li><strong>Python 3.14 is too new for demucs.</strong> Install with <code>pipx install demucs==4.0.1 --python python3.11</code>.</li>
          <li>First run downloads an ~80MB model. After that, processing takes ~50 seconds per song on M-series.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="music-generation">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">05</span>
          <h2>Music Generation</h2>
        </div>
        <p class="task-summary">Generate original songs from text prompts ‚Äî with lyrics, genre control, and full composition plans.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/music-gen">music-gen skill</a></p>
        <p>The <code>music-gen</code> skill uses the ElevenLabs Music API. Two modes: quick prompt ("make me a lo-fi beat") or a detailed composition plan with per-section lyrics and styles.</p>
        <div class="media-showcase">
          <p class="media-label">üéµ Lo-fi test beat (quick prompt mode)</p>
          <audio controls src="media/lofi-test.mp3"></audio>
          <p class="media-label">üé≠ "La Gaufre" ‚Äî an opera about waffles (composition plan mode)</p>
          <audio controls src="media/la-gaufre.mp3"></audio>
          <p class="media-caption">Two approaches: simple prompt vs. detailed composition plan with per-section lyrics and styles</p>
        </div>
        <h3>Quick mode</h3>
        <pre><code>bash scripts/music-gen.sh "upbeat indie rock about coding at 3am" --length 60</code></pre>
        <h3>Composition plan mode</h3>
        <p>For full control, write a JSON plan defining global styles, sections (verse/chorus/bridge), per-section lyrics, and durations. This is how we made "La Gaufre" ‚Äî a 30-second opera about waffles with a baritone singing over sparse piano.</p>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>No copyrighted references.</strong> Mentioning "Puccini-style" got rejected. The API suggests alternatives ‚Äî just strip the artist names and describe the sound instead.</li>
          <li>Composition plan gives much better results than simple prompts for anything specific.</li>
          <li>Use <code>negative_global_styles</code> to exclude what you don't want ‚Äî "no drums", "no choir", etc.</li>
          <li>Songs can be 3 seconds to 10 minutes. Sections: 3 seconds to 2 minutes each.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="music-videos">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">06</span>
          <h2>Music Videos</h2>
        </div>
        <p class="task-summary">Generate video scenes with Sora, stitch them over the audio with ffmpeg.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/music-gen">music-gen skill</a> (includes Sora video script)</p>
        <p>Once we had a song, we wanted visuals. OpenAI's Sora API generates 4/8/12-second video clips from text prompts. We generated 3 scenes in parallel and stitched them over the La Gaufre audio.</p>
        <div class="media-showcase">
          <video controls src="media/la-gaufre-video.mp4" poster="media/waffle-avatar.png"></video>
          <p class="media-caption">La Gaufre ‚Äî the music video. Three Sora-generated scenes stitched over the opera.</p>
        </div>
        <h3>The pipeline</h3>
        <ol>
          <li>Write scene prompts (cinematic, specific, visual)</li>
          <li>Submit all scenes in parallel via Sora API (async jobs)</li>
          <li>Poll for completion (~2-3 min each)</li>
          <li>Download the clips</li>
          <li>Stitch with ffmpeg: <code>ffmpeg -f concat -i list.txt -i song.mp3 -map 0:v -map 1:a -shortest output.mp4</code></li>
        </ol>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>Sora's moderation is aggressive.</strong> "Epic like a perfume commercial" got blocked. Keep prompts descriptive but neutral.</li>
          <li>The <code>/content</code> endpoint returns raw binary, not JSON with a URL ‚Äî download directly.</li>
          <li>Clips are exactly 12.1 seconds (not 12.0). Use <code>-shortest</code> flag to trim to the audio length.</li>
          <li>3 parallel 12-second clips = ~36 seconds of footage for a 30-second song. Gives you some buffer.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="youtube-summarize">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">07</span>
          <h2>YouTube Summarization</h2>
        </div>
        <p class="task-summary">Extract transcripts from YouTube videos and summarize them ‚Äî as text or voice.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/youtube-summary">youtube-summary skill</a></p>
        <p>The <code>summarize</code> CLI extracts transcripts from YouTube URLs. For long videos, we use <code>--extract-only</code> to get the transcript, then summarize in-context to avoid OOM kills.</p>
        <div class="media-showcase example-summary">
          <p class="media-label">üìù Example: TFTC podcast summary</p>
          <p class="summary-source">Source: <a href="https://www.youtube.com/watch?v=iDNlOGn1bTw">TFTC #647 ‚Äî Justin Moon on AI, Freedom Tech & Building with LLMs</a> (2,733-line transcript ‚Üí 3 paragraphs)</p>
          <audio controls src="media/tftc-summary.mp3"></audio>
          <p class="media-caption">Voice summary generated with ElevenLabs (George voice) ‚Äî same content as below</p>
          <blockquote>
            <p>Justin Moon is back after a year away from building. In that time, AI changed the game ‚Äî he sat down and in 24 hours, vibe-coding with LLMs, built what used to take him a month. But the real conversation goes deeper: the most important human trait going forward isn't intelligence, it's will. It's agency. If AI can code, write, and think, what matters is having the vision and drive to point it somewhere meaningful.</p>
            <p>They talk a lot about freedom tech ‚Äî Nostr, Bitcoin, running local models, self-hosting your AI so you're not dependent on some company that can pull the rug. Justin's been working on protocols like Marmet and White Noise (encrypted relay-based communication). And Marty, who's not a programmer, talks about using OpenClaw to handle real business tasks ‚Äî which is the whole point: you don't need to be a dev anymore to build things.</p>
            <p>The vibe is optimistic but grounded. They're honest about the existential weirdness of AI, but they see this as a moment to build tools that give people sovereignty.</p>
          </blockquote>
        </div>
        <h3>How it works</h3>
        <ul>
          <li><code>summarize --extract-only "youtube-url"</code> ‚Üí raw transcript</li>
          <li>Agent summarizes the transcript in-context (Claude handles this natively)</li>
          <li>Optionally: generate a voice memo summary using the voice-memo skill</li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>Full summarization OOMs on long videos.</strong> A 2,733-line podcast transcript gets killed. Use <code>--extract-only</code> and summarize manually.</li>
          <li>The <code>summarize</code> CLI handles YouTube natively ‚Äî no need for <code>yt-dlp</code> just for transcripts.</li>
          <li><code>yt-dlp</code> is only needed when you want the actual audio/video file (for stem splitting, etc.).</li>
        </ul>
      </div>
    </section>

    <section class="task" id="email-calendar">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">08</span>
          <h2>Email & Calendar</h2>
        </div>
        <p class="task-summary">Gmail and Google Calendar access via the gog CLI ‚Äî send emails, check inbox, create events with invites.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>The <code>gog</code> CLI (built-in OpenClaw skill) gives the agent access to Gmail, Calendar, and Contacts via Google OAuth. Waffle's first email was a recap of everything we built on day one.</p>
        <h3>What it can do</h3>
        <ul>
          <li><strong>Gmail:</strong> Search, read, send, reply, draft ‚Äî plain text or HTML</li>
          <li><strong>Calendar:</strong> List events, create events with attendees and colors, update/delete</li>
          <li><strong>Contacts:</strong> List and search contacts</li>
        </ul>
        <h3>Setup</h3>
        <ol>
          <li>Create a Google Cloud project and enable Gmail, Calendar, and People APIs</li>
          <li>Create OAuth credentials (Desktop app) ‚Üí download <code>client_secret.json</code></li>
          <li>OAuth consent screen must be <strong>External</strong> (not Internal) for regular Gmail accounts</li>
          <li><code>gog auth credentials /path/to/client_secret.json</code></li>
          <li><code>gog auth add you@gmail.com --services gmail,calendar,contacts</code> ‚Üí approve in browser</li>
        </ol>
        <h3>Spam strategy</h3>
        <p>If your agent's email gets a lot of spam (ours does), set up a whitelist of addresses to actually pay attention to. Everything else gets ignored. We check email via heartbeats (~every 30 min) and only alert on whitelisted senders.</p>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>OAuth "Internal" mode blocks regular Gmail.</strong> You'll get "Error 403: org_internal". Switch to External and add yourself as a test user.</li>
          <li><strong>APIs must be enabled individually</strong> in Google Cloud Console ‚Äî Gmail API, Calendar API, People API. You'll get a 403 with a helpful link if you forget one.</li>
          <li>Set <code>GOG_ACCOUNT=you@gmail.com</code> as an environment variable so you don't have to pass <code>--account</code> every time.</li>
          <li>Calendar invites need <code>--attendees "email@example.com"</code> to actually send the invite to someone.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="clamshell">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">09</span>
          <h2>Clamshell Mode</h2>
        </div>
        <p class="task-summary">Keep the MacBook running with the lid closed ‚Äî no external monitor needed.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/clamshell">clamshell skill</a></p>
        <p>If you're running OpenClaw on a laptop and want to close the lid without it sleeping, you need to disable clamshell sleep. One command:</p>
        <pre><code>sudo pmset -a disablesleep 1   # on
sudo pmset -a disablesleep 0   # off</code></pre>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>Requires sudo.</strong> The agent can't run this without elevated permissions or your password.</li>
          <li><strong>Turn it off when done!</strong> Your Mac won't sleep at all with the lid closed ‚Äî battery drain is real.</li>
          <li>Works on macOS Tahoe (26.x). No external display or keyboard required.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="this-website">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">10</span>
          <h2>This Website</h2>
        </div>
        <p class="task-summary">Meta: how an AI built, reviewed, and deployed the site you're reading right now.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/site-deploy">site-deploy skill</a></p>
        <p>Paul said "let's make a website that chronicles our work together." Waffle wrote the HTML and CSS, deployed it to Vercel, then used the Chrome extension to actually look at what it built and fix the layout.</p>
        <h3>The process</h3>
        <ol>
          <li>Wrote the site as plain HTML + CSS ‚Äî no framework, no build step</li>
          <li>Opened it in Chrome via the OpenClaw browser extension</li>
          <li>Took a screenshot, reviewed it, and noticed the layout was right-aligned (oops)</li>
          <li>Fixed the CSS, refreshed, re-screenshotted ‚Äî iteration loop just like a human developer</li>
          <li>Paul pointed out spacing issues from a screenshot ‚Äî fixed those too</li>
          <li>Deployed to Vercel with <code>npx vercel --prod</code></li>
          <li>Created a GitHub repo with <code>gh repo create</code> and pushed</li>
          <li>Added sanitized skills to the repo (stripped personal data, hardcoded paths)</li>
        </ol>
        <h3>Key insight</h3>
        <p>The Chrome extension was the unlock. Without it, Waffle was building a website blind ‚Äî writing HTML but never seeing the result. With it, the agent can screenshot, review, and iterate just like looking over someone's shoulder. The feedback loop went from "Paul, does this look right?" to "I can see it's broken, let me fix it."</p>
        <h3>Stack</h3>
        <ul>
          <li><strong>Site:</strong> Plain HTML + CSS, no framework, no build step</li>
          <li><strong>Hosting:</strong> Vercel (free tier, CLI deploy)</li>
          <li><strong>Source:</strong> <a href="https://github.com/waffledog-bot/paul-and-waffle">GitHub repo</a> with skills included</li>
          <li><strong>Design review:</strong> OpenClaw Chrome extension (browser screenshots)</li>
        </ul>
        <h3>Gotchas</h3>
        <ul>
          <li><strong>Vercel needs <code>vercel login</code> first.</strong> Can't deploy without it ‚Äî the agent can't do the OAuth flow for you.</li>
          <li><strong>GitHub too:</strong> <code>gh auth login</code> required before the agent can create repos and push.</li>
          <li>Media files (audio, video) add up fast ‚Äî this site is ~25MB mostly from stem MP3s and the music video.</li>
          <li>Keep skills sanitized before committing ‚Äî strip phone numbers, API keys, and hardcoded user paths.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="x-search">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">11</span>
          <h2>X/Twitter Research Tool</h2>
        </div>
        <p class="task-summary">Built a CLI for searching and analyzing X/Twitter discourse using Grok's x_search ‚Äî adapted from OpenUniverse.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>We wanted to do real research on X/Twitter ‚Äî not just search, but <em>analyze</em>. Topic sentiment, account positions, debate classification, all with proper citations. Inspired by Paul's earlier project <a href="https://github.com/AnthonyRonning/openuniverse">OpenUniverse</a>, we stripped it down to a single Python script that outputs markdown reports.</p>

        <h3>What it does</h3>
        <ul>
          <li><strong>Search:</strong> Find top/viral tweets on any topic with engagement metrics</li>
          <li><strong>Topic analysis:</strong> Define two "sides" of a debate, find tweets, classify them into camps (from OpenUniverse)</li>
          <li><strong>Account analysis:</strong> Map someone's positions across configurable topics with tweet evidence</li>
          <li><strong>Freeform ask:</strong> Natural language questions about any account, with cited sources</li>
          <li><strong>Citations:</strong> Every claim links to a real tweet URL ‚Äî no unsourced assertions</li>
        </ul>

        <h3>The approach</h3>
        <p>The key insight: xAI's Grok model has a built-in <code>x_search</code> tool that can natively search X/Twitter. No separate Twitter API bearer token needed ‚Äî just the xAI key. We use the <code>xai-sdk</code> Python package to send prompts with x_search enabled, and Grok does the searching and analysis in one shot.</p>
        <p>OpenUniverse is a full-stack app (FastAPI + React + PostgreSQL + Chrome extension). We took the core ideas ‚Äî topic classification, account analysis, side-by-side debate scoring ‚Äî and packed them into a ~300-line Python CLI that outputs clean markdown. No database, no frontend, no server.</p>

        <h3>First real test: Why is Bitcoin down?</h3>
        <p>We investigated Bitcoin's 50% crash (Oct 2025 ‚Üí Feb 2026) using a combo of web search (Brave API) for news articles and x-search for Twitter discourse. Found two competing narratives:</p>
        <ul>
          <li><strong>Camp A (Macro-driven):</strong> Tariffs, Fed hawkishness, geopolitical risk-off</li>
          <li><strong>Camp B (Crypto-structural):</strong> Overleveraged positions, liquidation cascades, ETF outflows, cycle top</li>
        </ul>
        <p>The topic analysis tool classified 15+ viral tweets and found <strong>Camp B dominated overwhelmingly</strong> (9:1). Best take came from <a href="https://x.com/chigrl/status/1990584897703195018">@chigrl</a>: <em>"Everyone is looking for a single cause when this is actually a systems failure with multiple transmission mechanisms reinforcing each other."</em></p>

        <h3>Key decisions</h3>
        <ul>
          <li><strong>Grok-4-1-fast only:</strong> Only the grok-4 family supports x_search. Tried grok-3-mini first ‚Äî instant error.</li>
          <li><strong>Citations mandatory:</strong> Every prompt enforces source linking. After our first draft report came back without links, we rewrote all prompts to demand <code>[text](url)</code> for every claim.</li>
          <li><strong>No database (yet):</strong> SQLite caching is a future enhancement. For now, every query is fresh from Grok. Simple &gt; complex.</li>
          <li><strong>Ask &gt; Topic for complex research:</strong> The two-step topic flow (find ‚Üí classify) can break on JSON parsing. A single detailed "ask" prompt with classification instructions works more reliably.</li>
        </ul>

        <h3>Gotchas</h3>
        <ul>
          <li><strong>Don't search official accounts.</strong> Searching @bitcoin or @cryptocurrency for "why is bitcoin crashing" returns low-signal news aggregation. The real discourse is from individual analysts and traders.</li>
          <li><strong>JSON parsing is fragile.</strong> Grok sometimes wraps JSON in markdown or returns slightly different formats. Our <code>extract_json()</code> helper handles most cases but isn't bulletproof.</li>
          <li><strong>Blended research works best:</strong> Web search for facts + X search for vibes + synthesis = the most useful output.</li>
          <li><strong>The <code>.env</code> file is more reliable than config:</strong> API keys in <code>~/.openclaw/.env</code> get picked up by the gateway process more reliably than <code>openclaw.json</code> config patches (which sometimes need a full restart).</li>
        </ul>

        <h3>Source</h3>
        <ul>
          <li><a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/x-search">x-search skill on GitHub</a></li>
          <li>Inspired by: <a href="https://github.com/AnthonyRonning/openuniverse">OpenUniverse</a></li>
        </ul>
      </div>
    </section>

    <section class="task" id="cashu-wallet">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">12</span>
          <h2>Cashu Ecash Wallet</h2>
        </div>
        <p class="task-summary">Bitcoin-backed ecash wallet for Lightning payments and privacy-preserving transactions ‚Äî powered by the cocod CLI.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>We set up a Cashu ecash wallet so Waffle can send and receive Bitcoin/Lightning payments. Cashu uses blind signatures for privacy ‚Äî the mint can't link who sent what to whom. The <code>cocod</code> CLI (installed from <a href="https://clawhub.com">ClawHub</a>) handles everything.</p>

        <h3>What is Cashu?</h3>
        <p>Cashu is a Chaumian ecash protocol. You deposit Bitcoin (via Lightning), and the mint gives you ecash tokens backed 1:1. These tokens can be transferred privately ‚Äî the mint signs them with blind signatures, so it can't trace transactions. Think digital cash that's actually private.</p>

        <h3>What it can do</h3>
        <ul>
          <li><strong>Lightning payments:</strong> Pay any Lightning invoice (bolt11) directly from the wallet</li>
          <li><strong>Ecash transfers:</strong> Send/receive Cashu tokens ‚Äî copy-paste a token string and it's done</li>
          <li><strong>NUT-24 (HTTP 402):</strong> Automatically pay for API access when a server returns <code>402 Payment Required</code> with an <code>X-Cashu</code> header</li>
          <li><strong>NPC Lightning addresses:</strong> Receive payments at a Lightning address tied to your Nostr pubkey</li>
          <li><strong>Balance & history:</strong> Check balance, view transaction history</li>
        </ul>

        <h3>Setup</h3>
        <pre><code># Install via bun
bun install -g cocod

# Initialize with a mint
cocod init --mint-url https://mint.minibits.cash/Bitcoin

# Check balance
cocod balance</code></pre>

        <h3>Gotchas</h3>
        <ul>
          <li><strong>Back up your mnemonic.</strong> The wallet generates a mnemonic on init ‚Äî store it securely (<code>chmod 600</code>). Lose it and your ecash is gone.</li>
          <li><strong>Mints are trusted custodians.</strong> Your ecash is only as good as the mint backing it. Use reputable mints.</li>
          <li><strong>Skill came from ClawHub</strong> (<code>clawhub install cocod</code>) ‚Äî community-maintained, version-pinned to the CLI release.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="fal-video">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">13</span>
          <h2>Video Generation (fal.ai)</h2>
        </div>
        <p class="task-summary">Text-to-video and image-to-video using fal.ai ‚Äî multiple models including Grok Imagine, Seedance, and Kling.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p class="skill-link">üì¶ <a href="https://github.com/waffledog-bot/paul-and-waffle/tree/main/skills/fal-video">fal-video skill</a></p>
        <p>The <code>fal-video</code> skill wraps fal.ai's queue API to generate videos from text prompts or animate still images. It supports multiple models ‚Äî each with different strengths ‚Äî so you can pick the right tool for the job.</p>

        <h3>Two modes</h3>
        <ul>
          <li><strong>Text-to-video:</strong> Describe what you want and get a video clip (4-12 seconds)</li>
          <li><strong>Image-to-video:</strong> Upload a still image + a motion prompt to animate it</li>
        </ul>

        <h3>Available models</h3>
        <ul>
          <li><strong>Grok Imagine Video</strong> (default) ‚Äî best overall quality as of Feb 2026</li>
          <li><strong>Seedance 1.5 Pro</strong> ‚Äî good audio generation, cinematic feel</li>
          <li><strong>Kling 3.0 Pro</strong> ‚Äî highest detail, largest file sizes</li>
          <li><strong>Veo 3.1</strong> ‚Äî Google's model, fixed 8-second duration</li>
          <li><strong>LTX-2 19B</strong> ‚Äî open source, high resolution (1408√ó768)</li>
        </ul>

        <h3>How it works</h3>
        <pre><code># Text-to-video
bash scripts/fal-video.sh text2video "A wolf howling at the moon" --model grok --duration 8

# Image-to-video (animate a photo)
bash scripts/fal-video.sh img2video "She turns and walks away" --image photo.jpg

# Check result (async queue)
bash scripts/fal-video.sh result &lt;request_id&gt; --model grok</code></pre>

        <h3>Gotchas</h3>
        <ul>
          <li><strong>Async queue model.</strong> You submit a job, get a request ID, then poll for the result. Generation takes 30s‚Äì3min depending on model and duration.</li>
          <li><strong>Different models, different strengths.</strong> Grok is the default for a reason, but Seedance handles audio better and Kling produces the most detailed output. Experiment.</li>
          <li><strong>Aspect ratios and resolution</strong> are configurable ‚Äî 16:9, 9:16 (vertical), 1:1 (square), etc. Default is 720p.</li>
          <li>Requires <code>FAL_KEY</code> environment variable.</li>
        </ul>
      </div>
    </section>

    <section class="task" id="suno-music">
      <div class="task-header" onclick="toggleTask(this)">
        <div>
          <span class="task-number">14</span>
          <h2>Suno (Browser-Automated Music)</h2>
        </div>
        <p class="task-summary">Generate music on Suno via browser automation ‚Äî idea mode, covers from audio references, and clip extensions.</p>
        <span class="expand-icon">+</span>
      </div>
      <div class="task-details">
        <p>Unlike the ElevenLabs music-gen skill (API-based), the Suno skill drives <a href="https://suno.com">suno.com</a> directly through browser automation. This gives access to Suno's full feature set ‚Äî including covers from audio references, which is great for turning voice memos or hums into full songs.</p>

        <h3>Three modes</h3>
        <ul>
          <li><strong>Idea:</strong> Text prompt only ‚Äî write lyrics and style tags, Suno generates the song. Simplest mode.</li>
          <li><strong>Cover:</strong> Upload an audio clip as a reference. Suno recreates it in a new style while trying to match the melody and structure. Perfect for turning a hummed melody or voice memo into a produced track.</li>
          <li><strong>Extend:</strong> Continue an existing Suno clip from where it left off ‚Äî make songs longer or create variations.</li>
        </ul>

        <h3>How it works</h3>
        <p>This is pure browser automation ‚Äî no API. The agent navigates to <code>suno.com/create</code>, switches to Custom mode, fills in lyrics and style tags, clicks Create, then polls until the clips are ready. Downloads from Suno's CDN.</p>

        <h3>Cover mode flow</h3>
        <ol>
          <li>Navigate to Create page, switch to Custom mode</li>
          <li>Click Audio ‚Üí Upload the reference audio file</li>
          <li>Trim if needed, select "Cover" condition type</li>
          <li>Optionally add lyrics and style tags (or let Suno auto-generate)</li>
          <li>Create ‚Üí wait ‚Üí download</li>
        </ol>

        <h3>Gotchas</h3>
        <ul>
          <li><strong>Requires browser login.</strong> The OpenClaw internal browser (<code>profile: openclaw</code>) must be logged into Suno first. No API key ‚Äî it's session-based.</li>
          <li><strong>Upload triggers native file picker.</strong> The browser upload works but leaves macOS's native file dialog open ‚Äî may need manual dismissal. Known issue we're still working around.</li>
          <li><strong>Two music tools, different strengths:</strong> ElevenLabs (music-gen) is better for precise composition plans with per-section control. Suno is better for covers, audio references, and when you want that "Suno sound."</li>
          <li>Clips download from <code>https://cdn1.suno.ai/{clip_id}.mp3</code> once ready (~60-90 seconds).</li>
        </ul>
      </div>
    </section>

  </main>

  <footer>
    <div class="container">
      <p>Built by Paul & Waffle üßá ‚Äî powered by <a href="https://openclaw.ai">OpenClaw</a> + Claude</p>
      <p class="footer-note">This site is itself a product of the setup it describes. Updated as we build more stuff.</p>
    </div>
  </footer>

  <script>
    function toggleTask(header) {
      const section = header.parentElement;
      section.classList.toggle('open');
    }
  </script>
</body>
</html>
